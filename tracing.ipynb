{
 "cells": [
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Updating registry at `~/.julia/registries/General`\n",
      "  Updating git-repo `git@github.com:JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Tracing-based Automatic Differentiation\n",
    "=======================================\n",
    "\n",
    "Machine learning primarily needs [reverse-mode AD](./backandforth.ipynb), and\n",
    "tracing / operator overloading approaches are by far the most popular way to\n",
    "it; this is the technique used by ML frameworks from Theano to PyTorch. This\n",
    "notebook will cover the techniques used by those frameworks, as well as\n",
    "clarifying the distinction between the \"static declaration\"\n",
    "(Theano/TensorFlow) and \"eager execution\" (Chainer/PyTorch/Flux) approaches to\n",
    "AD."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "derive_r (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "include(\"utils.jl\")"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Partial Evaluation\n",
    "------------------\n",
    "\n",
    "Say we have a simple implementation of $x^n$ which we want to differentiate."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "function pow(x, n)\n",
    "  r = 1\n",
    "  for i = 1:n\n",
    "    r *= x\n",
    "  end\n",
    "  return r\n",
    "end\n",
    "\n",
    "pow(2, 3)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We already know how to [differentiate Wengert lists](./intro.ipynb), but this\n",
    "doesn't look much like one of those. In fact, we can't write this program as a\n",
    "Wengert list at all, given that it contains control flow; and more generally\n",
    "our programs might have things like data structures or function calls that we\n",
    "don't know how to differentiate either.\n",
    "\n",
    "Though it's possible to generalise the Wengert list to handle these things,\n",
    "there's actually a simple and surprisingly effective alternative, called\n",
    "\"partial evaluation\". This means running some part of a program without\n",
    "running all of it. For example, given an expression like $x + 5 * n$ where we\n",
    "know $n = 3$, we can simplify to $x + 15$ even though we don't know what $x$\n",
    "is. This is a common trick in compilers, and Julia will often do it for you:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CodeInfo(\n\u001b[90m1 ─\u001b[39m %1 = (Base.add_int)(x, 15)\u001b[36m::Int64\u001b[39m\n\u001b[90m└──\u001b[39m      return %1\n) => Int64"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "f(x, n) = x + 5 * n\n",
    "g(x) = f(x, 3)\n",
    "\n",
    "code_typed(g, Tuple{Int})[1]"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This suggests a solution to our dilemma above. If we know what $n$ is (say,\n",
    "$3$), we can write `pow(x, 3)` as $((1*x)*x)*x$, which _is_ a Wengert\n",
    "expression that we can differentiate. In effect, this is a kind of compilation\n",
    "from a complex language (Julia, Python) to a much simpler one."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Static Declaration\n",
    "------------------\n",
    "\n",
    "We want to trace all of the basic mathematical operations in the program,\n",
    "stripping away everything else. We'll do this using Julia's operator\n",
    "overloading; the idea is to create a new type which, rather than actually executing\n",
    "operations like $a + b$, records them into a Wengert list."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "- (generic function with 170 methods)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "import Base: +, -\n",
    "\n",
    "struct Staged\n",
    "  w::Wengert\n",
    "  var\n",
    "end\n",
    "\n",
    "a::Staged + b::Staged = Staged(w, push!(a.w, :($(a.var) + $(b.var))))\n",
    "\n",
    "a::Staged - b::Staged = Staged(w, push!(a.w, :($(a.var) - $(b.var))))"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Actually, all of our staged definitions follow the same pattern, so we can\n",
    "just do them in a loop. We also add an extra method so that we can multiply\n",
    "staged values by constants."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for f in [:+, :*, :-, :^, :/]\n",
    "  @eval Base.$f(a::Staged, b::Staged) = Staged(a.w, push!(a.w, Expr(:call, $(Expr(:quote, f)), a.var, b.var)))\n",
    "  @eval Base.$f(a, b::Staged) = Staged(b.w, push!(b.w, Expr(:call, $(Expr(:quote, f)), a, b.var)))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The idea here is to begin by creating a Wengert list (the \"graph\" in ML\n",
    "framework parlance), and create some symbolic variables which do not yet\n",
    "have numerical values."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Main.##366.Staged(Wengert List\n, :y)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "w = Wengert()\n",
    "x = Staged(w, :x)\n",
    "y = Staged(w, :y)"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "When we manipulate these variables, we'll get Wengert lists."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": ":(2x + y)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "z = 2x + y\n",
    "z.w |> Expr"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Crucially, this works with our original `pow` function!"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": ":(((1x) * x) * x)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "w = Wengert()\n",
    "x = Staged(w, :x)\n",
    "\n",
    "y = pow(x, 3)\n",
    "y.w |> Expr"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The rest is almost too easy! We already know how to derive this."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "quote\n    y1 = 1x\n    y2 = y1 * x\n    y4 = x * x\n    (y2 + x * y1) + y4\nend"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "dy = derive_r(y.w, :x)\n",
    "Expr(dy)"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "If we dump the derived code into a function, we get code for the derivative\n",
    "of $x^3$ at any point (i.e. $3x^2$)."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "75"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "cell_type": "code",
   "source": [
    "@eval dcube(x) = $(Expr(dy))\n",
    "\n",
    "dcube(5)"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Congratulations, you just implemented TensorFlow."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Eager Execution\n",
    "---------------"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This approach has a crucial problem; because it works by stripping out control\n",
    "flow and parameters like $n$, it effectively freezes all of these things. We\n",
    "can get a specific derivative for $x^3$, $x^4$ and so on, but we can't get the\n",
    "general derivative of $x^n$ with a single Wengert list. This puts a severe\n",
    "limitation on the kinds of models we can express.$^1$\n",
    "\n",
    "The solution? Well, just re-build the Wengert list from scratch every time!"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "75"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "cell_type": "code",
   "source": [
    "function D(f, x)\n",
    "  x_ = Staged(w, :x)\n",
    "  dy = derive(f(x_).w, :x)\n",
    "  eval(:(let x = $x; $(Expr(dy)) end))\n",
    "end\n",
    "\n",
    "D(x -> pow(x, 3), 5)"
   ],
   "metadata": {},
   "execution_count": 12
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3125"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "cell_type": "code",
   "source": [
    "D(x -> pow(x, 5), 5)"
   ],
   "metadata": {},
   "execution_count": 13
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This gets us our gradients, but it's not going to be fast – there's a lot of overhead\n",
    "to building and evaluating the list/graph every time. There are two things we can\n",
    "do to alleviate this:\n",
    "\n",
    "1. Interpret, rather compile, the Wengert list.\n",
    "2. Fuse interpretation of the list (the numeric phase) with the building\n",
    "   and manipulation of the Wengert list (the symbolic phase).\n",
    "\n",
    "Implementing this looks a lot like the `Staged` object above. The key difference\n",
    "is that alongside the Wengert list, we store the numerical values of each variable\n",
    "and instruction as we go along. Also, rather than explicitly naming variables\n",
    "`x`, `y` etc, we generate names using `gensym()`."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Symbol(\"##367\")"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "cell_type": "code",
   "source": [
    "gensym()"
   ],
   "metadata": {},
   "execution_count": 14
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct Tape\n",
    "  instructions::Wengert\n",
    "  values\n",
    "end\n",
    "\n",
    "Tape() = Tape(Wengert(), Dict())\n",
    "\n",
    "struct Tracked\n",
    "  w::Tape\n",
    "  var\n",
    "end\n",
    "\n",
    "function track(t::Tape, x)\n",
    "  var = gensym()\n",
    "  t.values[var] = x\n",
    "  Tracked(t, var)\n",
    "end\n",
    "\n",
    "Base.getindex(x::Tracked) = x.w.values[x.var]\n",
    "\n",
    "for f in [:+, :*, :-, :^, :/]\n",
    "  @eval function Base.$f(a::Tracked, b::Tracked)\n",
    "    var = push!(a.w.instructions, Expr(:call, $(Expr(:quote, f)), a.var, b.var))\n",
    "    a.w.values[var] = $f(a[], b[])\n",
    "    Tracked(a.w, var)\n",
    "  end\n",
    "  @eval function Base.$f(a, b::Tracked)\n",
    "    var = push!(b.w.instructions, Expr(:call, $(Expr(:quote, f)), a, b.var))\n",
    "    b.w.values[var] = $f(a, b[])\n",
    "    Tracked(b.w, var)\n",
    "  end\n",
    "  @eval function Base.$f(a::Tracked, b)\n",
    "    var = push!(a.w.instructions, Expr(:call, $(Expr(:quote, f)), a.var, b))\n",
    "    a.w.values[var] = $f(a[], b)\n",
    "    Tracked(a.w, var)\n",
    "  end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 15
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Now, when we call `pow` it looks a lot more like we are dealing with normal\n",
    "numeric values; but there is still a Wengert list inside."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": ":(((1##368) * ##368) * ##368)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "cell_type": "code",
   "source": [
    "t = Tape()\n",
    "x = track(t, 5)\n",
    "\n",
    "y = pow(x, 3)\n",
    "y[]\n",
    "\n",
    "y.w.instructions |> Expr"
   ],
   "metadata": {},
   "execution_count": 16
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Finally, we need to alter how we derive this list. The key insight is that\n",
    "since we already have values available, we don't need to explicitly build\n",
    "and evaluate the derivative code; instead, we can just evaluate each instruction\n",
    "numerically as we go along. We more-or-less just need to replace our symbolic\n",
    "functions like (`addm`) with the regular ones (`+`).\n",
    "\n",
    "This is, of course, not a particularly optimised implementation, and faster\n",
    "versions have many more tricks up their sleaves. But this gets at all the key\n",
    "ideas."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(75,)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "cell_type": "code",
   "source": [
    "function derive(w::Tape, xs...)\n",
    "  ds = Dict()\n",
    "  val(x) = get(w.values, x, x)\n",
    "  d(x) = get(ds, x, 0)\n",
    "  d(x, Δ) = ds[x] = d(x) + Δ\n",
    "  d(lastindex(w.instructions), 1)\n",
    "  for v in reverse(collect(keys(w.instructions)))\n",
    "    ex = w.instructions[v]\n",
    "    Δ = d(v)\n",
    "    if @capture(ex, a_ + b_)\n",
    "      d(a, Δ)\n",
    "      d(b, Δ)\n",
    "    elseif @capture(ex, a_ * b_)\n",
    "      d(a, Δ * val(b))\n",
    "      d(b, Δ * val(a))\n",
    "    elseif @capture(ex, a_^n_Number)\n",
    "      d(a, Δ * n * val(a) ^ (n-1))\n",
    "    elseif @capture(ex, a_ / b_)\n",
    "      d(a, Δ * val(b))\n",
    "      d(b, -Δ*val(a)/val(b)^2)\n",
    "    else\n",
    "      error(\"$ex is not differentiable\")\n",
    "    end\n",
    "  end\n",
    "  return map(x -> d(x.var), xs)\n",
    "end\n",
    "\n",
    "derive(y.w, x)"
   ],
   "metadata": {},
   "execution_count": 17
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "With this we can implement a more general gradient function."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "gradient (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "cell_type": "code",
   "source": [
    "function gradient(f, xs...)\n",
    "  t = Tape()\n",
    "  xs = map(x -> track(t, x), xs)\n",
    "  f(xs...)\n",
    "  derive(t, xs...)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 18
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Even with the limited set of gradients that we have, we're well on our way to\n",
    "differentiating more complex programs, like a custom `sin` function."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(3, 2)"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "cell_type": "code",
   "source": [
    "gradient((a, b) -> a*b, 2, 3)"
   ],
   "metadata": {},
   "execution_count": 19
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "mysin (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "cell_type": "code",
   "source": [
    "mysin(x) = sum((-1)^k/factorial(1.0+2k) * x^(1+2k) for k = 0:5)"
   ],
   "metadata": {},
   "execution_count": 20
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0.8775825618898637,)"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "cell_type": "code",
   "source": [
    "gradient(mysin, 0.5)"
   ],
   "metadata": {},
   "execution_count": 21
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8775825618903728"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "cell_type": "code",
   "source": [
    "cos(0.5)"
   ],
   "metadata": {},
   "execution_count": 22
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We can even take nested derivatives!"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(-0.4794255386164159,)"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "cell_type": "code",
   "source": [
    "gradient(x -> gradient(mysin, x)[1], 0.5)"
   ],
   "metadata": {},
   "execution_count": 23
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-0.479425538604203"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "cell_type": "code",
   "source": [
    "-sin(0.5)"
   ],
   "metadata": {},
   "execution_count": 24
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Though the tracing approach has significant limitations, its power is in how\n",
    "easy it is to implement: one can build a fairly full-featured implementation,\n",
    "in almost any language, in a weekend. Almost all languages have the\n",
    "operator-overloading features required, and no matter how complex the host\n",
    "language, one ends up with a simple Wengert list."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note that we have not removed the need to apply our basic symbolic\n",
    "differentiation algorithm here. We are still looking up gradient definitions,\n",
    "reversing data flow and applying the chain rule – it's just interleaved with\n",
    "our numerical operations, and we avoid building the output into an explicit\n",
    "Wengert list.\n",
    "\n",
    "It's somewhat unusual to emphasise the symbolic side of AD, but I think it\n",
    "gives us an incisive way to understand the tradeoffs that different systems\n",
    "make. For example: TensorFlow-style AD has its numeric phase separate from\n",
    "Python's runtime, which makes it awkward to use. Conversely, PyTorch does run\n",
    "its numerical phase at runtime, but also its symbolic phase, making it\n",
    "impossible to optimise the backwards pass.\n",
    "\n",
    "We [observed](./forward.ipynb) that OO-based forward mode is particularly\n",
    "successful because it carries out its symbolic and numeric operations at\n",
    "Julia's compile and run time, respectively. In the [source to source reverse\n",
    "mode](./reverse.ipynb) notebook, we'll explore doing this for reverse mode as\n",
    "well."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Footnotes"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "$^1$ Systems like TensorFlow can also just provide ways to inject control flow\n",
    "into the graph. This brings us closer to a [source-to-source\n",
    "approach](./reverse.ipynb) where Python is used to build an expression in\n",
    "TensorFlows internal graph language."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Fun fact: PyTorch and Flux's tapes are actually closer to the `Expr` format\n",
    "that we originally used, in which \"tracked\" tensors just have pointers to\n",
    "their parents (implicitly forming a graph/Wengert list/expression tree). A\n",
    "naive algorithm for backpropagation suffers from exponential runtime for the\n",
    "*exact* same reason that naive symbolic diff does; \"flattening\" this graph\n",
    "into a tree causes it to blow up in size."
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0-DEV.177"
  },
  "kernelspec": {
   "name": "julia-1.2",
   "display_name": "Julia 1.2.0-DEV.177",
   "language": "julia"
  }
 },
 "nbformat": 4
}
